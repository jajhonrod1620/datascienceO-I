{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are the regions at the company complying with the forecast?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Business Context.** You work at O-I glass inc, the company is wondering how the compliance of the forecast is evolving in february in the regions in which it operates. The following are the regions that need to be analized.\n",
    "\n",
    "1. APAC.\n",
    "2. EU.\n",
    "3. Latin America.\n",
    "4. North America.\n",
    "\n",
    "Because the firm is quite large, good strategies to get the compliance of forecast are hard to come by. The leaders of the company are asking for insights related to compliance so they can come up with good strategies to get good results in all regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Problem.**  your team lead asks you to investigate the following: **\"How is the compliance in each region?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical Context.** The data you've been given is in the Comma Separated Value (CSV) format, and comprises shipped quantities and tonnes for each of the forementioned regions. This case begins with a brief overview of this data, after which you will: (1) learn how to use the Python library ```pandas``` to load the data; (2) use ```pandas``` transform this data into a form amenable for analysis; and finally (3) use ```pandas``` to analyze the above question and come to a conclusion. As you may have guessed, ```pandas``` is an enormously useful library for data analysis and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages to aid in data analysis\n",
    "\n",
    "External libraries (a.k.a. packages) are code bases that contain a variety of pre-written functions and tools. This allows you to perform a variety of complex tasks in Python without having to \"reinvent the wheel\" build everything from the ground up. We will use two core packages: ```pandas``` and ```numpy```.\n",
    "\n",
    "```pandas``` is an external library that provides functionality for data analysis. Pandas specifically offers a variety of data structures and data manipulation methods that allow you to perform complex tasks with simple, one-line commands.\n",
    "\n",
    "```numpy``` is a package that we will use later in the case that offers numerous mathematical operations. Together, ```pandas``` and ```numpy``` allow you to create a data science workflow within Python.\n",
    "\n",
    "Let's import both packages using the ```import``` keyword. We will rename ```pandas``` to ```pd``` and ```numpy``` to ```np``` using the ```as``` keyword. This allows us to use the short name abbreviation when we want to reference any function that is inside either package. The abbreviations we chose are standard across the data science industry and should be followed unless there is a very, very good reason not to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Import the NumPy package\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these packages are loaded into Python, we can use their contents. Let's first take a look at ```pandas``` as it has a variety of features we will use to load and analyze our stock data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of ```pandas```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At the core of the ```pandas``` library are two fundamental data structures/objects:\n",
    "1. ```Series```\n",
    "2. ```DataFrame```\n",
    "\n",
    "A ```Series``` object stores single-column data along with an **index**. An index is just a way of \"numbering\" the ```Series``` object. For example, in this case study, the indices will be dates, while the single-column data may be stock prices or daily trading volume.\n",
    "\n",
    "A ```DataFrame``` object is a two-dimensional tabular data structure with labeled axes. It is conceptually helpful to think of a DataFrame object as a collection of Series objects. Namely, think of each column in a DataFrame as a single Series object, where each of these Series objects shares a common index -  the index of the DataFrame object.\n",
    "\n",
    "Below is the syntax for creating a Series object, followed by the syntax for creating a DataFrame object. Note that DataFrame objects can also have a single-column â€“ think of this as a DataFrame consisting of a single Series object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple Series object\n",
    "simple_series = pd.Series(index=[0,1,2,3], name='Volume', data=[1000,2600,1524,98000])\n",
    "simple_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing ```pd.Series``` to ```pd.DataFrame```, and adding a columns input list, a DataFrame object can be created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple DataFrame object\n",
    "simple_df = pd.DataFrame(index=[0,1,2,3], columns=['Volume'], data=[1000,2600,1524,98000])\n",
    "simple_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame objects are more general compared to Series objects. Let's create a two column DataFrame object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create another DataFrame object\n",
    "another_df = pd.DataFrame(index=[0,1,2,3], columns=['Date','Volume'], data=[[20190101,1000],[20190102,2600],[20190103,1524],[20190104,98000]])\n",
    "another_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how a list of lists was used to specify the data in the ```another_df``` DataFrame. Each element of the list corresponds to a row in the DataFrame, so the list has 4 elements because there are 4 indices. Each element of the list of lists has 2 elements because the DataFrame has two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using <code>pandas</code> to analyze forecast data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recall that we have CSV files that include data for each of the following O-I Glass regions:\n",
    "\n",
    "1. Asia Pacific.\n",
    "2. Europe.\n",
    "3. Latin America.\n",
    "4. North America.\n",
    "\n",
    "The available data for each region includes:\n",
    "\n",
    "1. **Calendar Day:** The shipment date, only includes february current dates\n",
    "2. **Region:** Region in which the tonnes were shipped\n",
    "3. **Commercial Forecast Tonnes:** The forecasted value of tonnes to be shipped\n",
    "4. **Shipped Quantity:** The actual quantity of units shipped\n",
    "5. **Shipped Tonnes:** The actual number of tonnes shipped\n",
    "\n",
    "To get a better sense of the available data, let's first take a look at just the data for OI, listed on the Asia Pacific file. You are given a CSV file that contains the company's shipment data, ```apac.csv```. Pandas allows easy loading of CSV files through the use of the method ```pd.read_csv()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load a file as a DataFrame and assign to df\n",
    "df = pd.read_csv('data/apac.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the file ```apac.csv``` are now stored in the DataFrame object ```df```.\n",
    "\n",
    "There are several common methods and attributes available to take a peek at the data and get a sense of it:\n",
    "\n",
    "1. ```DataFrame.head()```  -> returns the column names and first 5 rows by default\n",
    "2. ```DataFrame.tail()```  -> returns the column names and last 5 rows by default\n",
    "3. ```DataFrame.shape```   -> returns (num_rows, num_columns)\n",
    "4. ```DataFrame.columns``` -> returns index of columns\n",
    "5. ```DataFrame.index```   -> returns index of rows\n",
    "\n",
    "Using ```df.head()``` and ```df.tail()``` we can take a look at the data contents. Unless specified otherwise, Pandas Series and DataFrame objects have indicies starting at 0 and increase monotonically upward along the integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the head of the DataFrame (i.e. the top rows of the DataFrame)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the tail of the DataFrame (i.e. the top rows of the DataFrame)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see there are 19 data entries (each with 4 data points) for OI. The shape of a DataFrame is accessed using the ```shape``` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the shape of the two-dimensional structure, that is (num_rows, num_columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that ```DataFrame.columns``` and ```DataFrame.index``` return an index object instead of a list. To cast an index to a list for further list manipulation, we use the ```list()``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the column names of the DataFrame\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of the column names of the DataFrame\n",
    "list(df.index)[0:10] # only showing first 10 index values so reduce screen output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating additional variables relevant to forecast compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes, the data provided to you will not be sufficient to achieve your goal. You may have to add additional variables or data features to assist you. Recall that our original question concerned the compliance within the different regions where the company operates. Therefore, our DataFrame must have features related to these quantities.\n",
    "\n",
    "It can be helpful to think about adding columns to DataFrames as adding adjacent columns one-by-one in Excel. Here is an example of how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column named \"Region\"\n",
    "df['Region'] = 'APAC'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access a column by using [] brackets and the column name\n",
    "df['Shipped Tonnes'].head() # added .head() to suppress output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column named \"Quantity_Thousands\", which is calculated from the Shipped Quantity column currently in df\n",
    "df['Quantity_Thousands'] = df['Shipped Quantity'] / 1000.0 # divide every row in df['Shipped Quantity'] by 1 thousand, store in new column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the updated DataFrame shape. Two new columns have been added.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, we need to have a feature in our DataFrame that is related to compliance. Because this currently does not exist, we must create it from the already available features. Recall that compliance is the division between the Shipped Tonnes and the Forecast, save the results in a new column called *Compliance* and print the dataframe head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the power of ```pandas```. We can simply perform mathematical operations on columns of DataFrames just as if the DataFrames were single variables themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have features relevant to the original question, and can proceed to the analysis step. A common first step in data analysis is to learn about the distribution of the available data. We will do this next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning about the data distribution through summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's aggregate summary statistics for the four regions of the company. Fortunately, the DataFrame and Series objects offer a myriad of data summary statistics methods:\n",
    "\n",
    "1. ```min()```\n",
    "2. ```median()```\n",
    "3. ```mean()```\n",
    "4. ```max()```\n",
    "5. ```quantile()```\n",
    "\n",
    "Below, each method is used on the ```Shipped Tonnes``` column. Notice how simple the functions are to apply to the DataFrame. Simply type the name of the DataFrame, followed by a ```.``` and then the method name you'd like to calculate. We've chosen to select a single column ```Shipped Tonnes``` from the DataFrame ```df```, but you could have just as easily called these methods on the full DataFrame rather than a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the minimum of the Shipped Tonnes column\n",
    "df['Shipped Tonnes'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median of the Shipped Tonnes column\n",
    "df['Shipped Tonnes'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average of the Shipped Tonnes column\n",
    "df['Shipped Tonnes'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the maximum of the Shipped Tonnes column\n",
    "df['Shipped Tonnes'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like to explore the data distribution at a more granular level to see how the distribution looks beyond the simple summary statistics presented above. For this, we can use the ```quantile()``` method. The ```quantile()``` method will return the value which represents the given percentile of all the data under study (in this case, of the ```Shipped Tonnes``` data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 25th percentile\n",
    "df['Shipped Tonnes'].quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 75th percentile\n",
    "df['Shipped Tonnes'].quantile(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a more efficient method to quickly compute all of these summary statistics? Yes. One incredibly useful method that combines these summary statistics and also adds a couple others is the ```describe()``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Shipped Tonnes'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Determine the 25th, 50th, and 75th percentile for the ```Commercial Forecast Tonnes```and ```Shipped Quantity``` columns of ```df```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is indicated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating data from multiple regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So far, we've only been looking at data from one of our four regions. Let's go ahead and combine all four CSV files to analyze the four regions together. This will also reduce the amount of programming work required since the code will be shared across the four regions.\n",
    "\n",
    "One way to accomplish this aggregation task is to use the ```pd.concat()``` method from ```pandas```. An input into this method may be a list of DataFrames that you'd like to concatenate. We will use a for loop to loop over each region name, load the corresponding CSV file, and then append the result to a list which is later aggregated using ```pd.concat()```. Let's take a look at how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load five csv files into one dataframe\n",
    "print(\"Defining region names\")\n",
    "regions_data_to_load = ['apac','eu','la','na']\n",
    "list_of_df = []\n",
    "\n",
    "# Loop over all regions names\n",
    "print(\" --- Start loop over regions --- \")\n",
    "for i in regions_data_to_load:\n",
    "    print(\"Processing Region: \" + i)\n",
    "    temp_df = pd.read_csv('data/'+i+'.csv')\n",
    "    temp_df['Region'] = i # ADD NEW COLUMN WITH REGION NAME TO DISTINGUISH IN FINAL DATAFRAME\n",
    "    list_of_df.append(temp_df)\n",
    "\n",
    "print(\" --- Complete loop over regions --- \")\n",
    "    \n",
    "# Combine into a single DataFrame by using concat\n",
    "print(\"Aggregating Data\")\n",
    "agg_df = pd.concat(list_of_df, axis=0)\n",
    "\n",
    "# Add salient statistics for Compliance\n",
    "print('Calculating Salient Features')\n",
    "agg_df['Compliance'] = (agg_df['Shipped Tonnes'] / agg_df['Commercial Forecast Tonnes'])\n",
    "\n",
    "print(\"agg_df DataFrame shape (rows, columns): \")\n",
    "print(agg_df.shape)\n",
    "\n",
    "print(\"Head of agg_df DataFrame: \")\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the for loop, we've aggregated and added the relevant features we identified in the previous section. We then printed the head of the aggregated DataFrame to have a peek at the format of the data, and we've also printed the shape of the DataFrame. This is to sanity check that our final DataFrame is roughly what we expect. Notice the aggregated DataFrame has the same number of columns as the original single region (APAC) data, however the number of rows have increased. This makes sense, because each additional region contains its own data entries. So, this passes our sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to reverse this process and extract the data relevant to a single stock symbol from the aggregated DataFrame ```agg_df```, we can do so using the ```==``` operator, which returns True when two objects contain the same value, and False otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_LA_df = agg_df[agg_df['Region'] == 'la']\n",
    "region_LA_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the code block above, we've filtered out the rows that correspond to each region. Namely,\n",
    "\n",
    "```python\n",
    "agg_df['Region'] == 'la'\n",
    "```\n",
    "returns a boolean series of the same number of rows of ```agg_df```, where each value is ```True``` or ```False``` depending on whether a specific row's ```Region``` values is equal to ```'la'```.\n",
    "\n",
    "This row extraction technique will be useful to us later in this case when we perform analyses on each individual region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Write code to write a for loop to loop through each of the four regions, extract only the rows correpsonding to each region, and calculate and print the average ```Shipped Quantity``` value for each of the four regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is indicated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "region_list = ['apac','eu','la','na']\n",
    "\n",
    "for i in region_list:\n",
    "   # code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing each region Compliance levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```pandas``` offers the ability to group related rows of DataFrames according to the values of other rows. This useful feature is accomplished using the ```groupby()``` method.  Let's take a look and see how this can be used to group rows so that each group corresponds to a single region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the groupby() method, notice a DataFrameGroupBy object is returned\n",
    "agg_df.groupby('Region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, the ```DataFrameGroupBy``` object can be most readily thought of as containing a DataFrame object for every group (in this case, a DataFrame object for each region). Specifically, each item of the object is a tuple, containing the group identifier (in this case the Region), and the corresponding rows of the DataFrame that have that Region).\n",
    "\n",
    "Fortunately, ```pandas``` allows you to iterate over the groupby object to see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grp_obj = agg_df.groupby('Region') # Group data in agg_df by Region\n",
    "\n",
    "# Loop through groups\n",
    "for item in grp_obj:\n",
    "    print(\" ------ Loop Begins ------ \")\n",
    "    print(type(item))     # Showing type of the item in grp_obj\n",
    "    print(item[0])        # Region\n",
    "    print(item[1].head()) # DataFrame with data for the Region\n",
    "    print(\" ------ Loop Ends ------ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the ```pd.groupby()``` method with the ```describe()``` method and apply it to each region to analyze the distribution of compliance related features for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grp_obj = agg_df.groupby('Region') # Group data in agg_df by Region\n",
    "\n",
    "# Loop through groups\n",
    "for item in grp_obj:\n",
    "    print('------Region: ', item[0])\n",
    "    grp_df = item[1]\n",
    "    relevant_df = grp_df[['Compliance']]\n",
    "    print(relevant_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One immediate observation of note is that the compliance level can vary widely and some regions tend to infinite. There are many reasons for this, a probable hypothesis is that we weren't expecting sales on these days but still got shipped tonnes.\n",
    "\n",
    "Another observation is that all regions got compliance levels over 80%.\n",
    "\n",
    "While this is great to see, there is a more powerful way to display this data in pandas. We can call the ```describe()``` method directly on the ```DataFrameGroupBy``` object. This one line allows you to avoid having to write a for loop every time you'd like to summarize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compliance\n",
    "agg_df[['Region','Compliance']].groupby('Region').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is identical to the data previously outputted using the for loop approach. The difference is that utilizing the features of the ```DataFrameGroupBy``` object allows for easy coding, fast results, and a clean output. This illustrates the power of using the ```pd.groupby()``` method: generating statistics for groups of interest in your data is straightforward and efficient to code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling data points as complying and not complying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've determined that the compliance levels of each region can vary widely per day, the next logical step is to group periods of complied, high compliance and low compliance to identify days with low compliance.\n",
    "\n",
    "However, we don't currently have a column that identifies when compliance is high and when it is low. Therefore, we must create a new column called ```Status``` using a threshold. For example, we'd like to have a new column value determined by:\n",
    "\n",
    "```\n",
    "\n",
    "if Compliance > treshold:\n",
    "    Status= 'High Compliance'\n",
    "else:\n",
    "    Status = 'Low Compliance'\n",
    "```\n",
    "\n",
    "Here we will define low compliance levels by any ```Compliance``` below the 50%th percentile. High compliance is over the 50%th percentile.\n",
    "Let's take a look how we can accomplish this task using ```groupby()``` functionality and the ```quantile()``` method, which returns the percentile for a given series of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine lower thresholds for volatility for each symbol\n",
    "status_thresholds = agg_df.groupby('Region')['Compliance'].quantile(0.5) # 50th percentile (median)\n",
    "print(status_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'd like to label periods of high and low volatility by symbol, we will make use of the ```np.where()``` method in the ```numpy``` library. This method takes an input and checks a logical condition: if the condition is true, it will return its second argument, whereas if the condition is false, it will return its third argument. This is very similar to how Microsoft Excel's ```IFERROR()``` method works (helpful to think of it this way for those familiar with Excel). Let's loop through each symbol and label each day as either high and low volatility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through regions\n",
    "print(\"Defining regions\")\n",
    "list_of_regions= ['apac','eu','la','na']\n",
    "list_of_df = []\n",
    "\n",
    "# Loop over all regions\n",
    "print(\" --- Loop over symbols --- \")\n",
    "for i in list_of_regions:\n",
    "    print(\"Labelling compliance for region: \" + i)\n",
    "    temp_df = agg_df[agg_df['Region'] == i].copy() # make a copy of the dataframe to ensure not affecting agg_df\n",
    "    volstat_t = status_thresholds.loc[i]\n",
    "\n",
    "    temp_df['Status'] = np.where(temp_df['Compliance'] < volstat_t, 'Low Compliance', 'High Compliance') # Compliance label\n",
    "    list_of_df.append(temp_df)\n",
    "    \n",
    "print(\" --- Completed loop over regions --- \")\n",
    "\n",
    "print(\"Aggregating data\")\n",
    "labelled_df = pd.concat(list_of_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labelled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now added a ```Status``` column that identifies whether each Region is in a period of high or low compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Write code to group regions into Low, Medium, or High compliance, where:\n",
    "\n",
    "```\n",
    "if Compliance > (75th percentile compliance for given region):\n",
    "    Status = 'HIGH'\n",
    "elif  Compliance > (25th percentile compliance for given region):\n",
    "    Status = 'MEDIUM'\n",
    "else:\n",
    "    Status = 'LOW'\n",
    "```\n",
    "\n",
    "Output a ```final_df``` DataFrame output grouped by Region, showing the mean Compliance for each Status category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "# Determine thresholds for compliance for each region\n",
    "compliance_thresholds_75 =  #code here # 75th percentile \n",
    "compliance_thresholds_25 =  #code here # 25th percentile \n",
    "\n",
    "# Loop through regions\n",
    "print(\"Defining Regions\")\n",
    "list_of_Regions= ['apac','eu','la','na']\n",
    "list_of_df = []\n",
    "\n",
    "# Loop over all regions\n",
    "print(\" --- Loop over Regions --- \")\n",
    "for i in list_of_Regions:\n",
    "   #code here\n",
    "    \n",
    "print(\" --- Completed loop over Regions --- \")\n",
    "\n",
    "print(\"Aggregating data\")\n",
    "#code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Write a script to find and print the day that has the highest shipped tonnes for each Region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n",
    "# Add a column for the day\n",
    "day_list = []\n",
    "for i in agg_df['Calendar Day']:\n",
    "    day_list.append(i[2:4])\n",
    "    \n",
    "agg_df['day'] = day_list\n",
    "\n",
    "# Group by Region, then loop through the group object to group by day and calculate shipped tonnes\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this case, we've learned the foundations of the ```pandas``` library in Python. We now know how to:\n",
    "\n",
    "1. Read data from files\n",
    "2. Aggregate and manipulate data using ```pandas```\n",
    "3. Analyze summary statistics and gather information from trends across time\n",
    "\n",
    "Going forward, we will be able to use ```pandas``` as a data analysis framework to build more complex projects and solve critical business problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://owensillinois.sharepoint.com/:b:/r/teams/DS4OI-One/Shared%20Documents/General/Resources/Data%20Wrangling%20with%20pandas%20.pdf?csf=1&e=s2fgrh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.w3resource.com/python-exercises/pandas/index.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas merging 101\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/53645882/pandas-merging-101"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
